---
title: "Words, text mining, some basic corpus linguistics"
author: "Steven Moran & Alena Witzlack-Makarevich"
date: "(`r format(Sys.time(), '%d %B, %Y')`)"
output:
  github_document:
    toc: true
bibliography: 'references.bib'    
---

# Setup

We use [R](https://www.r-project.org/) [@R] and the following [R packages](https://cran.r-project.org/web/packages/available_packages_by_name.html) [@tidyverse;@tidytext;@wordcloud;@gutenbergr].

```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(wordcloud)
```

Many of the examples are taken from or inspired by [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com) by [Julia Silge](https://juliasilge.com) and [David Robinson](http://varianceexplained.org).


# Data

We can access books from [Gutenberg](https://www.gutenberg.org) project via the R [gutenbergr package](https://cran.r-project.org/web/packages/gutenbergr/index.html). Who was [Gutenberg](https://en.wikipedia.org/wiki/Johannes_Gutenberg)? 

What can the `gutenbergr` package do?

```{r load_data}
?gutenbergr
```

When we look at the help, there are some functions native to this package for browsing the collection of books.

```{r}
gutenberg_metadata
gutenberg_authors
```

Let's grab [Moby Dick](https://en.wikipedia.org/wiki/Moby-Dick) by [Herman Melville](https://en.wikipedia.org/wiki/Herman_Melville).

On the web, it looks like this:

* https://www.gutenberg.org/files/2701/2701-h/2701-h.htm

Let's download it and have a look. What has the `gutenberg_download()` done?

```{r}
moby_dick <- gutenberg_download(2701)
moby_dick
```

Now, recall we want to work with [tidy data](https://r4ds.had.co.nz/tidy-data.html) typically. What is so-called tidy data?

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

How can we do that?

***

One way is that we can use other people's work, i.e., another library that someone has created, to process the data for us. Pro-tip: when you need something done, go look for it on the web before you build it from scratch!

We can use the [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) package for that. 

```{r}
help(tidytext)
```

It has an `unnest_tokens()` function.

```{r}
?unnest_tokens
```


```{r}
tidy_moby_dick <- moby_dick %>%
  unnest_tokens(word, text)
```

Now what does our data structure look like?

```{r}
tidy_moby_dick
```

This took has *tokenized* the data for us, i.e., it has split the raw data (sentences in parapgrahs in chapters in a book) into words.

*Tokenization* is the process of splitting text into tokens.

This one-token-per-row structure is in contrast to the ways text is often stored, perhaps as strings or in a document-term matrix.

For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

Now compare the first and tokenized data. What's are the differences?

```{r}
moby_dick
tidy_moby_dick
```

What happened in the `unnest_tokens()` function?

```{r}
moby_dick
moby_dick %>%
  unnest_tokens(word, text)
```

What else happened?

Hint: bag of words.

Remember, use the power of the force. Help `?` is your friend -- in all its forms.

```{r}
?unnest_tokens
help(unnest_tokens)
```


# Exploration

What can we do with this "bag of words"?

One thing we can do is ask R to calculate the number words (tokens) in the novel. 

```{r}
length(tidy_moby_dick$word)
```

Another thing we can do is is ask R to calculate the number unique words (types) in the novel. 

```{r}
length(unique(tidy_moby_dick$word))
```

R's `unique()` function will examine all the values in the character vector (word column) and identify those that are the same and those that are different. By embedding the `unique()` function into the `length()` function, you calculate the number of unique word types from all the word tokens, i.e., all the unique words in Melville's Moby Dick vocabulary.

We can also ask R to `count()` the elements in the column for us. 

What do we see here? What do we have to pass to the `count()` function?

```{r}
tidy_moby_dick %>% 
  count(word)
```

Most functions have parameters and we can tell them what to do with certain variables or properties. How to know what they are? Ask for help!

```{r}
?count
```

There's a parameter in `sort()` that is by default set to `FALSE`, i.e., if we do not tell the function `count()` explicitly, `count(sort = TRUE)`, it will assume that it should NOT sort the count. 

So what happens if we set `sort = TRUE`?

```{r}
tidy_moby_dick %>% 
  count(word, sort = TRUE)
```

That's interesting, perhaps we want to save the results into a new data frame that we can call by a new variable name.

```{r}
moby_dick_word_counts <- tidy_moby_dick %>% 
  count(word, sort = TRUE)

moby_dick_word_counts
```

Now we have a data frame with two columns that includes and two data types:

* word
* n

How can we visualize these data?

* https://www.data-to-viz.com

*** 

Let's consider the column `n` (a common acronym used for "count"). What kind of data type is it? 

What kind of visualizations can we make with one numeric data point?

```{r}
moby_dick_word_counts %>% 
  filter(n > 600) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL)
```

This data visualization -- a [bar chart](https://en.wikipedia.org/wiki/Bar_chart) aka bar plot or bar graph tells us what?

Is it useful as is?

How can we display the information more meaningfully for the reader?

```{r}
tidy_moby_dick %>%
  count(word, sort = TRUE) %>% 
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```

```{r}
tidy_moby_dick %>%
  count(word, sort = TRUE) %>% 
  filter(n > 600) %>%
  ggplot(aes(x = reorder(word, -n), y = n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

What's another fun way to visualize one variable data?

```{r}
tidy_moby_dick %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 600))
```

***

We can also do a so-called dot plot. Try it!

```{r}
# Hint: use geom_point()
moby_dick_word_counts %>%
  filter(n > 500) %>% 
  ggplot(aes(x = reorder(word, -n), y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


***

What if we're interested in particular words?

How many occurrences of the word `whale` are in the book?

```{r}
moby_dick_word_count <- tidy_moby_dick %>% 
  count(word, sort = TRUE)

moby_dick_word_count %>% filter(word == "whale")
```

But is this a lot?

The raw count does NOT tell us much in terms of the validity of some hypothesis.

For example, is the word whale *much* more common in Moby Dick than in, say, some other works? How about Jane Austen?

We cannot say, because frequency is a *relative* judgement.

One way to put this number "of whales" into perspective is to express it as a percentage of the whole corpus, i.e., Moby Dick as a novel is over 200k words.

What percentage does the word for whale make out of the entire book, or in corpus linguistics speak, the "corpus"?

```{r}
whale_count <- moby_dick_word_count %>% filter(word == "whale")
```

But is that a lot, relatively speaking, compared to other words?

***

Let's express the words as a percentage of the whole corpus in which we see the words.

How many words do we have in the whole book / corpus?

```{r}
length(tidy_moby_dick$word)
```

Now we can use the total number of words as our denominator and we can extend our data frame by adding a new column.

```{r}
moby_dick_word_count$frequency <- moby_dick_word_count$n / length(tidy_moby_dick$word) * 100
moby_dick_word_count
```

Now we can again look at how frequent *within* the text a given word is.

```{r}
moby_dick_word_count %>% filter(word == "whale")
moby_dick_word_count %>% filter(word == "harpoon")
```


# Word type-token ratio (TTR)

The typeâ€“token ratio is one of the basic corpus statistics.

Comparing the number of tokens in the text to the number of types of tokens (unique word form) can tell us how large a range of vocabulary is used in the text.

* TTR = (number of types/number of tokens), or
* TTR = (number of types/number of tokens) * 100 (as a percentage)

TTR allows us to measure vocabulary variation between corpora: the closer the result is to 1 (or 100%), the greater the vocabulary variation.

Here's our tidy data.

```{r}
tidy_moby_dick
```

Now let's get the TTR.

```{r}
types <- length(unique(tidy_moby_dick$word))
tokens <- length(tidy_moby_dick$word)
ttr <- types / tokens
ttr
```

So now we can compare two (or more) texts to see which text has a greater range of vocabulary. For example, consider the vocabulary of rappers:

* https://pudding.cool/projects/vocabulary/index.html


# Comparing vocabulary between corpora

Now let's compare the vocabulary range of Melville with, say, Jane Austin -- or some other book of your choosing.

This should get you started.

```{r}
# You can replace with some other book -- maybe something at random? :)
sense_sensibility <- gutenberg_download(161)
sense_sensibility
```

Which text has the greater range in vocabulary in terms of its type-to-token ratio? 

Put your code here:

```{r}
tidy_sense_sensibility <- sense_sensibility %>%
  unnest_tokens(word, text)

types <- length(unique(tidy_moby_dick$word))
tokens <- length(tidy_moby_dick$word)
ttr <- types / tokens
ttr

types <- length(unique(tidy_sense_sensibility$word))
tokens <- length(tidy_sense_sensibility$word)
ttr <- types / tokens
ttr
```

How does the type-toke ratio between the two source look?

***

But note we can also drill down into a specific words and compare them!


# Is Moby Dick all about men?

Let's compare the (relative) frequencies of pronouns between texts for the pronouns "he" and "she".

Start first by calculating their normalized frequencies. Choose an appropriate normalization base.

Next, filter for all relevant pronouns (e.g. with the notation %in% c(...) or other filter options). Hint:

```{r}
tidy_moby_dick %>% filter(word %in% c('he', 'his'))
```

Compare the pronouns' relative frequencies between texts. Write your code here:

```{r}
# Insert your code
```

What about Moby Dick versus some other text? 

For comparison, load another text, pre-process it, and calculate the normalized frequency of the two sets of pronouns.

```{r}
# Insert your code here
```


# Ngrams

[N-grams](https://en.wikipedia.org/wiki/N-gram) is a sequence of n tokens (usually words or characters), which typically come from a text or corpus. Typically terms:

- unigram: one token
- bigram: two tokens
- trigram: three tokens

And so forth.

They are used in [language models](https://en.wikipedia.org/wiki/Language_model), which are are [probabilistic distribution](https://en.wikipedia.org/wiki/Probability_distribution) over sequences of words. Language models are used in computational linguistics for lots of things, e.g.:

- machine translation
- speech recognition
- part-of-speech tagging
- optical character recognition and hand writing recognition
- information retrieval (e.g., search engines)

The `unnest_tokens()` function takes both other types tokenizations and other lengths of tokenization.

```{r}
moby_dick_bigrams <- moby_dick %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

moby_dick_bigrams
```

How do we filter (out) the NAs? We can use the `is.na()` function.

```{r}
moby_dick_bigrams <- moby_dick %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

moby_dick_bigrams
```

Let's count them.

```{r}
moby_dick_bigrams %>% count(bigram, sort = TRUE)
```

What do you notice about the words?

We can filter out so-called [stop words](https://en.wikipedia.org/wiki/Stop_word), i.e., words that we want to filter out of our data because they for example carry very little meaning, such as "the", "of", etc.

To make this process easier, we first separate the bigrams into different columns with the `separate()` function. (Tip: there's also a `unite() function that will unite two columns!)

```{r}
bigrams_separated <- moby_dick_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_separated
```

Now let's filter.

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
```

And generate new counts.

```{r}
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

How can we visualize this type of data?

```{r}
# Heatmap 
temp <- bigram_counts %>% filter(n >= 30)

ggplot(temp, aes(word1, word2, fill= n)) + 
  geom_tile()
```



# Sentiment analysis

Now we can use this format to look at certain contexts.

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

And we can do some simple [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) on text, e.g., looking at polarity.

For more information, check out this chapter in the [Text Mining with R](https://www.tidytextmining.com/sentiment.html) book [@SilgeRobinson2017].

Sentiment analysis involves a lexicon that has been defined for values such as positivity versus negativity. First we need sentiment lexicon.

```{r}
AFINN <- get_sentiments("afinn")

AFINN
```

So let's look at the words preceded by "not" and their associated sentiment.

```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```

Now we can visualize the results.

```{r}
not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")
```

The bigrams "not die" and "not killed" may be giving the text a more negative sentiment. For example, we could compare this to another text and compare them.

We can also look at more negation words.

```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)
```

```{r}
negated_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(30) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by negation") +
  facet_grid(~word1)
```


# References

