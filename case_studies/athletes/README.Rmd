---
title: "Linear regression example: athletes height vs. weight"
author: "Steve Moran"
date: "(`r format(Sys.time(), '%d %B, %Y')`)"
output:
  github_document:
      toc: true
---

***

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
```

***

# Overvierw

Statistical modeling is an attempt to describe some part of the real world in mathematical terms.

The relevant mathematical concept is the one of **function**. 

Consider for example the input of `height` and output of `weight` with our `atheletes` data.


# Load the data

Let's load the data.

```{r, warning=FALSE, message=FALSE}
athletes <- read_csv('../../datasets/athletes.csv')
```

And have a look at it.

```{r}
head(athletes) %>% 
  kable()
```


# Define your hypothesis

Recall any hypothesis testing follows these steps, broadly speaking:

1. Choose a significance level (α)
2. Formulate a null hypothesis, H0
3. Formulate an alternative hypothesis, H1
4. Gather data, calculate a test statistic, e.g. T or F
5. Determine the probability (p-value) of obtaining T or F "or a more extreme value" under H0
6. If p ≤ α, reject H0

And the basic process is:
  
* Set up a hypothesis, and assume that it is true.
* Gather data from some real-world experiment that is relevant to the hypothesis.
* Make a determination about the hypothesis, based on the idea of "how likely is our data given the hypothesis?"

Let's go through an example.

***

First, let's set up our hypothesis regarding height vs weight of the athletes.

* H0:
* H1:

We have data already, let's look at what data types that we have. And here's its structure:

```{r}
str(athletes)
```

Recall our discussion on [data types in statistics](https://github.com/bambooforest/IntroDataScience/tree/main/3_data#data-types-in-statistics).


# Choose a statistical test

What kind of statistical test can we use to test if there's a relationship between these two variables?

Recall that the independent variable (predictor variable) is the variable that is being manipulated, so that we can observe if there is an effect on the dependent variable (outcome variable).

* Independent variable(s) -- Predictor variable(s)
* Dependent variable(s) -- Outcome/Output variable(s)

One way to look at that relationship is to **plot the input** on the x-axis and the output on the y-axis in a scatter plot.

```{r}
ggplot(athletes, aes(height, weight)) +
  geom_point()
```

One way to test whether there is a relationship with two continuous data points is to use [linear regression](https://en.wikipedia.org/wiki/Linear_regression).

```{r}
ggplot(athletes, aes(height, weight)) +
  geom_point() +
  geom_smooth(method='lm')
```


# Check your model assumptions

But hold on. Just because we see a pattern does not necessarily mean it's there. We have to be careful of statistical bias. How can we control for bias?

Like other statistical tests, **you will have to know the (four) main assumptions** for linear regression, i.e.:

* [Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) of observations (aka no [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation))
* [Normality](https://en.wikipedia.org/wiki/Normal_distribution)
* [Linearity](https://en.wikipedia.org/wiki/Linear_regression)
* [Homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) (aka homogeneity of variance)

Two events are **independent** if the occurrence of one event does not affect the chances of the occurrence of the other event.

We test for **normality** to see whether a set of data is distributed in a way that is consistent with a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), when our statistical test requires that the data points are normally distributed. 

**Linearity** is demonstrated when the mean values of the outcome variable (dependent variable) for each increment of the predictors (independent variables) lies along a straight line.

**Homoscedasticity** (homogeneity of variances) is an assumption of equal or similar variances in different groups being compared. 

<!--
[Parametric statistical test](https://en.wikipedia.org/wiki/Parametric_statistics) assume that data points in the sample come from a population that can be modeled by a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution) (i.e., a mathematical function that describes the probabilities of occurrence of the outcomes in an experiment) and that has a fixed set of [statistical parameters](https://en.wikipedia.org/wiki/Statistical_parameter). This assumption is important because parametric statistical tests are sensitive to any dissimilarities and uneven variance in samples will bias and skew the results.
-->

Here is a good and simple overview of linear models:

* https://www.scribbr.com/statistics/linear-regression-in-r/

***

1. Independence

For height and weight, we only have one independent variable and one dependent variable for each athlete, so we don’t need to test for any hidden relationships among the variables. In other words, we have independence of observations.

2. Normality

We need to check if the dependent variable is normally distributed. We can quickly visualize it. Does it look normal?

```{r}
hist(athletes$height)
```

Another way to look for normality is with a quantile-quantile plot (Q-Q plot), which is a graphical tool to assess whether the data come from some probability distribution. It is a scatter plot that plots two sets of [quantiles](https://en.wikipedia.org/wiki/Quantile) against each other. If for example, we have a relatively straight line, we may assume that our data points come from a normal distribution.

```{r}
qqnorm(athletes$height, pch = 1, frame = FALSE) # Create the Q-Qplot
qqline(athletes$height, col = "steelblue", lwd = 2) # Add a blue line for reference
```

3. Linearity

As we saw above, the data are linearly distributed. Here's another way to quickly visualize the x and y variables.

```{r}
plot(weight ~ height, data = athletes)
```


4. Homoscedasticity (aka homogeneity of variance)

This means that the prediction error doesn’t change significantly over the range of prediction of the model. We can test this assumption later, after fitting the linear model.


# Perform the linear regression analysis

So now we can do our linear regression and interpret the results.

```{r}
lm <- lm(weight ~ height, data = athletes)
summary(lm)
```

***

Can we say there is a significant positive relationship between height and weight of these athletes (p-value < 0.001).

***


# Check for homoscedasticity

```{r}
par(mfrow=c(2,2))
plot(lm)
par(mfrow=c(1,1))
```

Residuals are the unexplained variance. They are not exactly the same as model error, but they are calculated from it, so seeing a bias in the residuals would also indicate a bias in the error.

The most important thing to look for is that the red lines representing the mean of the residuals are all basically horizontal and centered around zero. This means there are no outliers or biases in the data that would make a linear regression invalid.

In the Normal Q-Qplot in the top right, we can see that the real residuals from our model form an almost perfectly one-to-one line with the theoretical residuals from a perfect model.

Based on these residuals, we can say that our model meets the assumption of homoscedasticity.

*** 
Statistical tests are about interpreting data. If we want to interpret our data with formal procedures and to make claims about the distribution of our data or whether two data sets differ fundamentally from each other, then we rely on hypothesis testing.
